{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cfcbb5",
   "metadata": {},
   "source": [
    "## Derivada\n",
    "\n",
    "### Derivada una variable \n",
    "\n",
    "El cálculo es una herramienta que nos que nos va a permitir estudiar como cambian las funciones con respecto a sus variables. Una función intuitivamente es una especie de máquina, que recibe valores de entrada y los utiliza para generar un valor de salida, a dichos valores de entra es a los que llamaremos variables de la función.\n",
    "\n",
    "<img src=\"Función-intuición.png\" width = 200 height = 200> \n",
    "\n",
    "Para hacernos una idea ce como cambia una función con respecto a sus variables vamos a fijarnos en la pendiente de la recta tangente a la gráfica de la función en un punto. podemos ver que entre más inclinada esta la recta tangente, es decir mayor es su pendiente, mayor es el cambio en la función.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Derivada en una variable.png\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "Para poder obtener el valor de dicha pendiente nos apoyaremos del concepto de _límite,_ tomemos dos puntos sobre la gráfica de la función y unamos dichos puntos por una recta, esta recta es una aproximación a la recta tangente, y podemos ver que no es una muy buena, pero noten que si dejamos fijo un punto y desplazamos el otro hacia el primero, entre más cerca estén dichos puntos mejor es la aproximación a la tangente, y de hecho en el límite la aproximación es perfecta, es justamente a dicho límite al que llamaremos la derivada de una función en un punto.\n",
    "\n",
    "\n",
    "<img src=\"Aproximación a la tangente.gif\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "\n",
    "$$f^{\\prime }(x)=\\lim _{\\Delta x \\to 0}{\\frac{f(x+ \\Delta x)-f(x)}{\\Delta x}}$$\n",
    "\n",
    "\n",
    "En la formulación llamamos $\\Delta x$ a la distancia que separa a dichos puntos, cuando la distancia de separación $\\Delta x$ tiende a 0 obtenemos la derivada. La podemos entender como la tasa de cambio de la variable $y$ en función de $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257d92f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a3e6c1",
   "metadata": {},
   "source": [
    "$$f^{\\prime }(x)=\\lim _{\\Delta x \\to 0}{\\frac{f(x+ \\Delta x)-f(x)}{\\Delta x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b154a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0f4fd3b",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f(x, y)}{\\partial x}=\\lim _{\\Delta x \\to 0}{\\frac{f(x+ \\Delta x, y)-f(x, y)}{\\Delta x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31015da",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial f(x, y)}{\\partial y}=\\lim _{\\Delta y \\to 0}{\\frac{f(x, y+ \\Delta y)-f(x, y)}{\\Delta y}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a1fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08f1893e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h3>Derivada Parcial (varias variables)</h3>\n",
    "\n",
    "En el apartado anterior vimos la derivada para funciones que únicamente dependen de una sola variable, pero este concepto lo podremos extender a funciones que dependan de más de una variable, para esto vamos a apoyarnos en el siguiente ejemplo:\n",
    "\n",
    "- Imaginemos que queremos describir la masa de una lata cilíndrica, aquí hay muchos parámetros en juego como el radio de la base $r$, la altura del cilindro $h$, el grosor de la lata $t$ y la densidad del material $\\rho$.\n",
    "\n",
    "<img src=\"Área del cilindro.gif\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "Podemos ver que el área de las caras del cilindro está dada por $A = 2 \\pi r^2 + 2 \\pi r h$, si conocemos el área de las caras del cilindro conocer su volumen tan sencillo como multiplicar por el grosor $t$, y una vez que se conoce el volumen del cilindro (note que no se habla del cilindro armado, si no al volumen de sus paredes), para conocer su masa solo hace falta multiplicar por la densidad $\\rho$\n",
    "\n",
    "$$m = 2 \\pi r^2 t \\rho + 2 \\pi r h t \\rho$$\n",
    "\n",
    "\n",
    "Ahora parece que $m$ depende de una cantidad considerable de variables ($r$, $h$, $t$, $\\rho$), y en principio es así. Pero aquí es donde nosotros haremos algunas consideraciones para simplificar el problema. Empecemos por asumir que toda la hoja de material de donde crearemos la lata tiene el mismo grosor en todos los puntos, es decir $t$ es un valor constante, y tambien podemos asumir que la lámina es homogénea y por lo tanto su densidad $\\rho$ también es constante. De esta forma hemos reducido el problema a uno de solo dos variables: $m(r, h)$.\n",
    "\n",
    "\n",
    "Y ahora nos preguntamos ¿cómo calcular una derivada de esta función para saber cómo cambia la masa con respecto a la altura y el radio?. Y la respuesta es sencilla, si queremos conocer $ \\frac{\\partial m}{\\partial r} $ supondremos que $h$ es constante y realizaremos la derivada como si $m(r)$ solo tuviera una variable, y viceversa.\n",
    "\n",
    "Dentro de <code>Sympy</code> podemos indicar esto agregando un parámetro al método diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfbbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e87c1f61",
   "metadata": {},
   "source": [
    "Noten que de esta forma podemos responder a preguntas como:\n",
    "\n",
    "- ¿Cuál es la masa de una lata de altura 10 cm y radio 3 cm?\n",
    "- ¿Qué tanto aumenta la masa si aumentamos 2 cm la altura?\n",
    "- ¿Y si aumentamos 0.5 el radio?\n",
    "- ¿Y si cambian ambos?\n",
    "\n",
    "suponiendo $t = 0.1cm$ y $\\rho = 7.85 \\frac{g}{cm3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627011be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171bc4ce",
   "metadata": {},
   "source": [
    "### Derivada total\n",
    "\n",
    "Supongamos que tenemos una función de 3 variables $f(x, y, z)$ y que a la vez $x(t)$, $y(t)$, $z(t)$, y supongamos que queremos conocer $\\frac{df}{dt}$, entonces el resultado será sumar las reglas de la cadena de $x$, $y$, y $z$.\n",
    "\n",
    "$$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x}\\frac{d x}{d t} + \\frac{\\partial f}{\\partial y}\\frac{d y}{d t} + \\frac{\\partial f}{\\partial z}\\frac{d z}{d t}$$\n",
    "\n",
    "Tengan en cuenta que, si en sympy nosotros definimos en un principio $x$, $y$, y $z$ como funciones de $t$ entonces el método diff calculara la derivada inmediatamente sin pasos intermedios.\n",
    "\n",
    "\n",
    "También podemos cálcular la diferencial total:\n",
    "\n",
    "$$df = \\frac{\\partial f}{\\partial x}dx + \\frac{\\partial f}{\\partial y}dy + \\frac{\\partial f}{\\partial z}dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503582e",
   "metadata": {},
   "source": [
    "$$dm = \\frac{\\partial m}{\\partial r} dr + \\frac{\\partial m}{\\partial h} dh$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7b54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17e1f248",
   "metadata": {},
   "source": [
    "## Gradiente\n",
    "\n",
    "\n",
    "Dada un campo escalar $f(x, y, z)$ vamos a llamar gradiente al vector $\\vec{\\nabla}f$ que tiene por componentes las derivadas parciales de $f$ con respecto de $x$, $y$, y $z$.\n",
    "\n",
    "$$\\vec{\\nabla}f\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\frac{\\partial f}{\\partial z}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Por convención lo denotaremos como un vector columna. Si evaluamos $\\vec{\\nabla}f$ en cualquier punto de $f$ obtendremos un vector que apunta en la dirección de mayor cambio de $f$ y cuya magnitud nos indica la tasa de cambio en esa dirección.\n",
    "\n",
    "\n",
    "Tomemos como ejemplo la función $f(x, y) = -\\frac{4x}{x^2 + y^2 + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f121393",
   "metadata": {},
   "source": [
    "### Ejercicio\n",
    "\n",
    "\n",
    "- Hacer una función que calcule el gradiente de una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcba6b4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (620984548.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    def f(x,y):\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def f(x,y):\n",
    "    return -(4*x)/x**2 + y**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4092cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bce093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3863e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2674f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f2131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba27172",
   "metadata": {},
   "source": [
    "## Matriz Jacobiana\n",
    "\n",
    "\n",
    "La matriz jacobiana es una matriz formada por las derivadas parciales de primer orden de una función. En este sentido, el jacobiano representa la derivada de una función multivariable. La matriz jacobiana extiende la definición del gradiente a [campos vectoriales](#Glosario), para entenderlo veamos el caso más sencillo, el jacobiano de una función $f : \\mathbb{R}^n \\to \\mathbb{R}$, en este caso particular $f$ es un [campo escalar](#Glosario) y el jacobiano tiene la siguiente forma:\n",
    "\n",
    "\n",
    "$$\\mathbf{J}_f(x_1, x_2, \\cdots, x_n) \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\cdots & \\frac{\\partial f}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Podemos notar que en este caso la matriz jacobiana es un vector renglón que coincide con $\\vec{\\nabla}f$, cabe destacar que la matriz jacobiana es la transpuesta de $\\vec{\\nabla}f$ en esta notación.\n",
    "\n",
    "Ahora extendamos el concepto a un [campo vectorial](#Glosario) $f : \\mathbb{R}^n \\to \\mathbb{R}^m$. Esta función está determinada por m funciones escalares reales:\n",
    "\n",
    "$$y_{i}=F_{i}(x_{1},\\ldots ,x_{n})$$\n",
    "$$\\vec{y}=\\vec {F}(\\vec{x})=(F_{1}(\\vec{x}),\\dots ,F_{m}(\\vec{x}))$$\n",
    "\n",
    "\n",
    "En este caso la matriz jacobiana toma la siguiente forma:\n",
    "\n",
    "\n",
    "$$\\mathbf{J}_\\vec{f(\\vec{x})}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Pongamos un ejemplo:\n",
    "\n",
    "$$F(x_1, x_2, x_3) = (x_1x_2x_3, x_2 - x_3^3)$$\n",
    "\n",
    "Esta es un [campo vectorial](#Glosario) $f : \\mathbb{R}^3 \\to \\mathbb{R}^2$, noten que podemos simplificar la notación haciendo $\\vec{x} \\in  \\mathbb{R}^3$ tal que $\\vec{x} = (x_1, x_2, x_3)$, de esta forma:\n",
    "\n",
    "$$\\vec{F}(\\vec{x}) = \\vec{y}$$\n",
    "\n",
    "Donde $\\vec{y} = (y_1, y_2)$ y $y_1 =  x_1x_2x_3,$, $y_2 = x_2 - x_3^3$\n",
    "\n",
    "Entonces la matriz jacobiana de F es:\n",
    "\n",
    "$$\\mathbf{J}_\\vec{F(\\vec{x})}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} &  \\frac{\\partial y_1}{\\partial x_3}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} &  \\frac{\\partial y_2}{\\partial x_3}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25065220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d22cb6",
   "metadata": {},
   "source": [
    "## Matriz Hessiana\n",
    "\n",
    "\n",
    "\n",
    "Para hablar de la matriz Hessiana vamos a regresar a hablar de [campos escalares](#Glosario), la matriz hessiana de una función $f$ de $n$ variables tal que $f : \\mathbb{R}^n \\to \\mathbb{R}$, es la matriz cuadrada de $n \\times n$, de las segundas derivadas parciales.\n",
    "\n",
    "\n",
    "$$\\mathbf{H}_{f(\\vec{x})}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Hay que notar que si las derivadas $\\frac{\\partial^2 f}{\\partial x_i x_j}$ existen y son continuas, entonces $\\frac{\\partial^2 f}{\\partial x_i x_j} = \\frac{\\partial^2 f}{\\partial x_j x_i}$ y la matriz hessiana es simétrica.\n",
    "\n",
    "\n",
    "Ejemplo: $-x^2 -y^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad8e49",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial^2 f}{\\partial x \\partial x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d64894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8592f766",
   "metadata": {},
   "source": [
    "### Matriz Hessiana y puntos críticos\n",
    "\n",
    "La matriz hessiana nos va a ser de utilidad para poder saber si un [punto crítico](#Glosario) de nuestra función es un máximo, un mínimo, o un punto silla, para esto nos debemos de fijar en la matriz hessiana evaluada en el [punto crítico](#Glosario):\n",
    "\n",
    "- si $det(H) < 0$ es un punto silla.\n",
    "- si $det(H) > 0$ entonces tenemos un máximo o un mínimo.\n",
    "    - si el primer elemento $h_{11}$ de la matriz es positivo entonces tenemos un mínimo.\n",
    "    - si el primer elemento $h_{11}$ de la matriz es negativo, entonces tenemos un máximo.\n",
    "    \n",
    "- si $det(H) = 0$ entonces hay duda.\n",
    "\n",
    "veamos un par de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb71c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e27f198",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dx} f(g(x)) = \\frac{df}{dg} \\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7eea75",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<h2>Regla de la cadena varias variables</h2>\n",
    "\n",
    "\n",
    "Cuando se tiene una composición de funciones de varias variables la regla de cadena cambia un poco\n",
    "\n",
    "en el caso de una función $f(x_1(t), x_2(t), \\cdots, x_n(t))$ se tiene que:\n",
    "\n",
    "$$ \\frac{d}{dt}f = \\frac{\\partial f}{\\partial x_1} \\frac{dx_1}{dt} + \\frac{\\partial f}{\\partial x_2} \\frac{dx_2}{dt} +  \\cdots + \\frac{\\partial f}{\\partial x_n} \\frac{dx_n}{dt}$$\n",
    "\n",
    "\n",
    "Si recordamos la sección de álgebra lineal notaremos que podemos escribir la regla de la cadena como sigue:\n",
    "\n",
    "$$ \\frac{d}{dt}f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}\\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\frac{dx_1}{dt}\\\\\n",
    "\\frac{dx_2}{dt}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{dx_n}{dt}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ \\frac{d}{dt}f = \\vec{\\nabla} f \\cdot \\frac{d}{dt} \\vec{x}$$\n",
    "\n",
    "\n",
    "También usando el concepto de la matriz jacobiana podemos escribir este caso de la regla de la cadena como sigue:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{d}{dt}f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\cdots & \\frac{\\partial f}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{dx_1}{dt}\\\\\n",
    "\\frac{dx_2}{dt}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{dx_n}{dt}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Podemos escribir la regla de la cadena de una forma más similar a la definición de una variable:\n",
    "\n",
    "$$ \\frac{d}{dt}f =  \\mathbf{J}_f \\frac{\\vec{dx}}{dt}$$\n",
    "\n",
    "En este caso la regla de la cadena la estamos expresando como el producto de dos matrices.\n",
    "\n",
    "Ahora supongamos que tenemos una función $f(\\vec{x}(\\vec{u}(t)))$, $f : \\mathbb{R}^n \\to \\mathbb{R}$, $\\vec{x}$ es una función tal que $x : \\mathbb{R}^m \\to \\mathbb{R}^n$ y por ultimo $\\vec{u}$ una función $u : \\mathbb{R} \\to \\mathbb{R}^m$. Para poner un ejemplo supongamos que $\\vec{x} = (x_1, x_2)$ y $\\vec{u} = (u_1(t), u_2(t))$. Al agregar un eslabón más a la cadena vamos a incrementar en un termino más la expresión de la regla de la cadena, sería algo como lo siguiente:\n",
    "\n",
    "\n",
    "$$ \\frac{d}{dt}f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial x_1}{\\partial u_1} & \\frac{\\partial x_1}{\\partial u_2}\\\\\n",
    "\\frac{\\partial x_2}{\\partial u_1} & \\frac{\\partial x_2}{\\partial u_2}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{du_1}{dt}\\\\\n",
    "\\frac{du_2}{dt}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\frac{d}{dt}f =  \\mathbf{J}_f \\mathbf{J}_x \\frac{\\vec{du}}{dt}$$\n",
    "\n",
    "\n",
    "La regla de la cadena tomara importancia en un futuro si se trabaja con redes neuronales. Pero, ¿cómo funcionan? A menudo verás diagramas como éste, donde los círculos son neuronas, y las líneas son la red de conexiones entre ellas. Esto podría sonar lejano a los temas que hemos cubierto hasta ahora, pero fundamentalmente, una red neuronal es tan solo una función matemática que recibe una variable y devuelve otra variable, donde ambas variables podrían ser vectores. Veamos ahora el caso más simple posible, de tal manera que podamos traducir estos diagramas en algunas fórmulas. \n",
    "\n",
    "\n",
    "<img src=\"red una neurona.png\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "\n",
    "Aquí tenemos una red, la cual toma una variable escalar única a la que llamaremos a0, y devuelve otro escalar a1. Podemos escribir esta función así: \n",
    "\n",
    "$$a^{(1)} = \\sigma (w a^{(0)} + b)$$\n",
    "\n",
    "Donde $b$ y $w$ son constantes, pero $\\sigma$ es ella misma una función. En este punto es útil dar a cada uno de estos términos un nombre, ya que ayudará a seguirlos cuando las cosas se compliquen un poco más. Los términos a se llaman actividades, $w$ es un peso, $b$ es un umbral y sigma es lo que llamamos la **función de activación**. Sigma es lo que permite asociar a las redes neuronales con el cerebro. Las neuronas en el cerebro reciben información de sus vecinos mediante estimulación química y eléctrica. Y cuando la suma de todas estas estimulaciones va más allá de un cierto umbral, la neurona repentinamente se activa y comienza a su vez a estimular a sus vecinos. Un ejemplo de función que tiene esta propiedad umbral es la función tangente hiperbólica, tanh, la cual es una función continua que va de menos uno a uno. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451987c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d0f90c0",
   "metadata": {},
   "source": [
    "$$a^{(1)} = \\sigma (w_0 a^{(0)} + w_1 a_1^{(0)} + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e62143",
   "metadata": {},
   "source": [
    "La tanh pertenece a una familia de funciones similares, todas con esta forma de S característica, llamadas sigmoideas. Por esta razón usamos sigma para este término. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Red neuronal.png\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "Podemos ver que al incrementar el número de neuronas la red se complica, pero la regla de la cadena cobra relevancia para poder saber cómo cambia una neurona con respecto a las variables de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153abfad",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h2>Optimización</h2>\n",
    "<h3>Descenso del gradiente</h3>\n",
    "\n",
    "Vamos a hablar de optimización, que es la gran aplicación del cálculo en los temas que nos interesaran, el primer método que veremos es el descenso del gradiente. Si el gradiente señala la dirección de mayor crecimiento de la función $f$, es natural pensar que si lo _seguimos_ nos llevará al valor máximo de $f$, y si vamos en contra de él nos conducirá a un mínimo, si es que estos puntos existen. Esta idea es la que va detrás del método del _descenso del gradiente_ que nos permite optimizar funciones no restringidas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4efc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a778d18",
   "metadata": {},
   "source": [
    "La idea es seria la siguiente:\n",
    "\n",
    "$$ x_1 = x_0 - \\alpha \\nabla f(x_0)$$\n",
    "\n",
    "\n",
    "¿Qué es lo que esta fórmula nos quiere decir?, bueno para empezar notemos que es un método iterativo, es decir lo debemos aplicar por pasos, empezaremos en una posición aleatoria, y el siguiente punto será la posición del anterior menos una fracción del gradiente, esta resta es equivalente a moverse en la dirección contraria a la que apunta el gradiente, es decir en dirección a un hipotético mínimo.\n",
    "\n",
    "Geométricamente ocurre algo como esto:\n",
    "\n",
    "<img src=\"Descenso del gradiente 1 variable.png\" width = 400 height = 400>\n",
    "\n",
    "<img src=\"Descenso del gradiente.gif\" width = 400 height = 400>\n",
    "\n",
    "\n",
    "Podemos ver que $\\alpha$ es un factor que nos da en cierto sentido el tamaño del paso que damos en cada iteración, si $\\alpha$ es muy pequeño tardaremos mucho en llegar al mínimo, o si por lo contrario es demasiado grande puede que nunca lleguemos al mínimo. Al ser un método iterativo nosotros le tenemos que decir cuando parar, lo hacemos cuando:\n",
    "\n",
    "$$ |x_n - x_{n - 1}| < \\epsilon $$\n",
    "\n",
    "Nosotros decidimos que tan pequeño debe ser $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f015f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4c9321",
   "metadata": {},
   "source": [
    "## Ejercicio RL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6094d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47ed4184",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Podemos aproximar la función $\\sin(x)$ por un polinomio de grado 5en el intervalo $(-3, 3)$, este polinomio de forma general tiene la siguiente forma:\n",
    "\n",
    "$$p(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + a_5 x^5$$\n",
    "\n",
    "Podemos utilizar el descenso del gradiente para encontrar los coeficientes $(a_0, a_1, a_2, a_3, a_4, a_5)$ que minimicen los errores entre nuestro polinomio y $\\sin(x)$, para ello nos apoyaremos del método de mínimos cuadrados que nos define la siguiente función:\n",
    "\n",
    "$$J(a_0, a_1, a_2, a_3, a_4, a_5) = \\int_{-3}^{3} (p(x) - \\sin(x))^2dx$$\n",
    "\n",
    "\n",
    "Si minimizamos la función $J$ encontraremos los coeficiente que buscamos.\n",
    "\n",
    "- __Use el método del descenso del gradiente para encontrar los coeficientes $(a_0, a_1, a_2, a_3, a_4, a_5)$ y haga una función que se llame *my_sin* que calcule aproximaciones del seno en el intervalo $(-3, 3)$ usando el polinomio de grado 5__\n",
    "\n",
    "- __Grafique $\\sin(x)$ y el polinomio encontrado para ver que tan buena es la aproximación.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6792b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
